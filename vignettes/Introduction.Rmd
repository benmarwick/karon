---
title: "Introduction to the karon package"
author: "John Karon & Ben Marwick"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 7,
  comment = "#>"
)
```

## Introduction

A common problem in archaeology is estimating or predicting the source of the material used to form an artifact.  Archaeologists may use 2- or 3-dimensional scatterplots of artifact chemical composition data to predict the source of an artifact.  This package contains a suite of R functions to apply multivariate statistical methods to this problem, as well as functions to create analysis data sets, carry out some basic data quality checks, and perform several descriptive statistical analyses.  Results are returned as tables or plots; appropriate tables can be saved as excel files, and the user can interact with some plots to identify data observations yielding points suggesting the need for closer identification.

The package implements three multivariate statistical procedures: principal component analysis (yielding 2-dimensional plots), recursive partitioning (classification trees), and random forests.  It may be of interest that classification trees and random forests can use both quantitative (e.g. chemical composition) and qualitative (e.g. color) data in the same analysis.

After creating analysis files and carrying out standard data quality and descriptive analyses, our suggested analysis strategy is the following: first, use these methods to evaluate whether the data available are sufficient to distinguish among sources.  Second, use a random forest analysis to order the characteristics according to their importance in distinguishing among sources.  Third, supply these characteristics in the specified order to a classification tree model, in order to predict the source of each artifact.  Finally, if all characteristics used are quantitative, plot the artifact points on a 2-dimensional principal component plot with the convex hulls of the source points, to identify artifacts for which the predicted sources are not plausible.

The final section of this vignette contains useful summary information on the functions in this package, including arguments that are standard among the functions, names of components of the list returned by each function, and instructions for selecting points of interest in some of the plots.  We also provide information on the differences between interacting with plots in base R versus Rstudio.  For those using Rstudio, we provide simple instructions for running the example code for a function.

## Example data

The example data available with this package are data on obsidian from five sources in the Jemez caldera in northern New Mexico and on obsidian artifacts from sites in the Pojoaque Valley east of that caldera.  For both the source and artifact samples, our example analyses use five elements (rubidium, Rb; strontium, Sr; ytterbium, Y; niobium, Nb; and zirconium, Zr; numerical values are in parts per million) analyzed by Steven Shackley using x-ray fluorescence.  These are the elements that Shackley uses to distinguish among sources and predict the source of an artifact.  The source data are publicly available (http://www.swxrflab.net/jemez.htm).  This site also contains discussions of the geology of the source sites.

We selected data on 91 artifacts from approximately 450 artifacts collected under the supervision of James Moore, Office of Archaeological Studies, New Mexico state government.  Shackley predicted the sources of these artifacts; all except two were from the Jemez caldera (we omitted those two from consideration); we randomly reassigned the sources of a proportion of the artifacts we selected in order to introduce misclassification.  These artifact data have not been published.

The artifacts were analyzed in Shackley’s lab in Albuquerque, New Mexico, using different instrumentation than that used for the sources.  For a discussion, see
http://www.swxrflab.net/labfees.htm#QuanX%20Energy-Dispersive%20X-Ray%20Fluorescence%20Spectrometer%20(EDXRF).  For a more general discussion of Shackley’s procedures, see http://www.swxrflab.net/swobsrcs.htm.

The five obsidian sources are coded as A, B, C, D, and E in the source and artifact data sets (ObsidianSources and ObsidianArtifacts, respectively).  The source (or predicted source for an artifact) is in the character-valued variable Code.  Each data set contains the variables Code, the five elements (as numeric variables, with variable names the element symbols), and a character-valued variable ID containing an artificial lab ID.  For example, the first few rows of ObsidianSources are

```{r}
library(karon)
data(ObsidianSources)
head(ObsidianSources)
names(ObsidianSources)
```
## Analysis file creation and check

The following procedure produces an analysis file from excel data files, with the first row of each file containing the name of the variable represented by the data in that column.  All files must have the same name for an element; if some files use “Zr” and others use “Zr “, the latter becomes “Zr.” when an R object is created.  Also, variable names in R are case-sensitive, so “Zr” and “ZR” are different variables.  In addition, each value of an element analysis must be numeric; the value “< 0” is not allowed.  Missing values should be left blank.

To create an R object (a data frame) from an excel file, use the R command

object_name <- read.csv(file = “file name”)

Other R functions could also be used, including read.table(); see the documentation for read.csv (obtained by entering ?read.csv at the R prompt) for alternatives and details.

If it is necessary to combine several data files into an analysis file, use the function fn.data() to create an R data frame from each of the individual R objects.  For example, Shakley’s website contains an excel file with data for individual sources.  After creating an R data frame for a source using e.g. read.csv(), the element dataOut of the list created by fn.data() is a data frame with the analysis variables in a specified sequence; to create this new data frame, use the command

new_object_name <- fn.data(data=object_name)$dataOut

where object_name is the data frame created from read.csv(); alternatively, save the result of fn.data() as an R object (a list), and create the new data frame with the command

new_object_name <-list_name$dataOut

These data frames can then be combined into an analysis data frame using the R function rbind().  If it may be necessary to repeat this operation for a number of source files, it may be useful to define an R function  

fn.combine <- function()
rbind(data1, data2,…..)

This function will return the result of the rbind() operation to define a data frame, such as

analysisFile <- fn.combine()

After creating the analysis file, use 

fn.CheckData(data=analysisFile, CheckDupVars, GroupVar, Groups, AnalyticVars) 

to obtain basic descriptive statistics and carry out several data checks (number of observations, number of missing values, number of negative values, duplicate observations, and descriptive statistics, by group and analysis variable).  For the data set ObsidianSources, there are no negative values for the five elements of interest and no duplicate observations.  The numbers of observations by source and element are

```{r}
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
CheckData<-fn.CheckData(data=ObsidianSources,CheckDupVars=c("Code","ID"),GroupVar="Code",Groups="All",AnalyticVars=analyticVars)
CheckData$Nvalues

```
An example of the descriptive statistics is  

```{r} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
CheckData<-fn.CheckData(data=ObsidianSources,CheckDupVars=c("Code","ID"),GroupVar="Code",Groups="All",AnalyticVars=analyticVars)
head(CheckData$Summary)

```
The example table shows that, in the descriptive statistics data frame (Robject$Summary), there is a row for each element and each source, with a row containing missing values (NAs) between elements.  If this data frame is written to an excel file, those rows between elements can be converted to blank rows, to make the resulting table easier to read.

## Descriptive statistics

The package contains functions to create basic descriptive statistics tables and plots: box plots, pairs plots (a matrix of two-dimensional scatterplots), and coefficients of variation and correlations between pairs of elements.

Box plots, created by the function fn.BoxPlots(), are useful for comparing the distributions of an element among sources and for identifying outlying values.  The figure shows box plots for four elements for the source data.  To understand the information in a plot, look at the plot of zirconium at source C.  The heavy line, at the narrow part of the notch, is the median of the zirconium values at C.  The top and bottom of the box represent the 25th and 75th percentiles of the data, respectively (the quantiles).  The notches define a 95% confidence interval for the median; if these do not overlap for two sources, it is plausible that those sources have significantly different medians.  (These plots yield warning messages that some of the notches should extend beyond the quartiles; these messages can be ignored.) Vertical dashed lines from the quartiles to a larger or smaller value with a horizontal line define the ranges of “adjacent values”.  The largest adjacent value is the largest value less than or equal to the upper quartile plus 1.5 times the interquartile range (the upper quartile minus the lower quartile).  The smallest adjacent value is the smallest value greater than or equal to the lower quartile minus 1.5 times the interquartile range.  For a standard normal distribution (mean 0, standard deviation 1), the median is 0, the upper and lower quartiles are 0.68 and -0.68, respectively; the upper and lower adjacent values are 2.72 (4 x 0.68) and -2.72, respectively.  The probability that a value from standard Gaussian distribution is greater than 2.72 is 0.003.  It follows that, for Gaussian data, the probability that a value lies outside the range of adjacent values is less than 0.01.  Therefore, values outside the range of adjacent values either may be considered outliers, or the data is unlikely to be Gaussian.   

If the box and the adjacent values are approximately symmetric around the median, then the distribution of the data is approximately symmetric.  Relative variation for values for an element among sources can be evaluated by comparing heights of boxes among sources.  
the boxes   

Now look at the figure.  We see immediately that there is little variation among the values of an element at a source. There are a few outliers, but they are not far from the range of adjacent values.  There is considerable variation in the distributions of these alements among the five Jemez sources, so it should be possible to distinguish these sources using these elements.  

```{r} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr")
boxPlots<-fn.BoxPlots(data=ObsidianSources, GroupVar="Code",                Groups="All",AnalyticVars=analyticVars,Nrow=2,Ncol=2)

```


## Pairs plots

We can make a pairs plot. A pairs plot is a matrix containing bivariate scatter plots for all pairs of quantitative variables.  The matrix is symmetric, so corresponding plots across the main diagonal interchange the horizontal and vertical axes.  Each plot contains a robust locally weighted line to describe the trend in the data.  This line is obtained from the R function `lowess()` which computes a predicted value at each abscissa (x-value) from a regression based on a fraction of the data, with weights decreasing at abscissas farther from the point and rejecting outliers.  This function tends to try to have the line go through or near the points with the largest and smallest abscissas; therefore the behavior at the ends of the plot is not reliable.  The line is obtained from the function `panel.smooth()` within the call to `pairs()` with an argument span.  The line (smoother) will not be useful if there are very few values; an example plot demonstrates that trends may show useful results even with 15 values.  

The plots are obtained with the function `fn.PairsPlot()` and the arguments data, GroupVar, Groups, AnalyticVars, and Span.  All except Span have the same meanings as in the function fn.BoxPlot.  Span is a value between 0 and 1 (not equal to 0) defining the proportion of data used to estimate robust smooth; the function is written with a default value of 2/3, which can be changed.  A small value (such as 0.1) will produce plots which do not show a clear trend; a large value (close to 1) will produce plots which do not show changes in trends.  Running the function will produce warnings that span is not a graphical parameter; these warnings should be ignored.


## Coefficients of variation for analyses and correlations between pairs of elements

These descriptive statistics can be obtained from the function fn.CV.corr() using the following arguments:    

- doc: character, default is "fn.CV.corr" data, GroupVar, Groups, AnalyticVars, and folder: these have the same meanings as for previous functions;    
- Transpose: if T (the default value), one row for the correlations between each pair of analyses (columns are groups)’ if F, one row for each group (columns are correlations between a pair of analyses);    
- CV.digits: number of significant digits in the table of coefficients of variation,
	default value is 2    
- corr.digits: number of significant digits in the table of correlations, 	default value is 2    
- folder: the path to the folder containing the excel files (Windows)      
- ds.CV: file name for coefficients of variation, with extension .csv    
- ds.corr: file name for Spearman correlation coefficients, with extension .csv    

This function returns a list with elements doc, CV (the coefficients of variation), and corr (the correlations).  By default, both the coefficients of variation and the correlations are rounded to two decimal places.  The coefficient of variation is the standard deviation of a set of data divided by the mean; it would be affected by outliers.  The Spearman correlation is the correlation based on ranks; it does not affected by outliers.  It is easier to make comparisons reading down a column than reading across a row, to this table makes it easier to see the variation in correlations among the sources.



