---
title: "Introduction to the karon package"
author: "John Karon & Ben Marwick"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 7,
  comment = "#>"
)
```

## Introduction

A common problem in archaeology is estimating or predicting the source of the material used to form an artifact.  Archaeologists may use 2- or 3-dimensional scatterplots of artifact chemical composition data to predict the source of an artifact.  This package contains a suite of R functions to apply multivariate statistical methods to this problem, as well as functions to create analysis data sets, carry out some basic data quality checks, and perform several descriptive statistical analyses.  Results are returned as tables or plots; appropriate tables can be saved as excel files, and the user can interact with some plots to identify data observations yielding points suggesting the need for closer identification.

The package implements three multivariate statistical procedures: principal component analysis (yielding 2-dimensional plots), recursive partitioning (classification trees), and random forests.  It may be of interest that classification trees and random forests can use both quantitative (e.g. chemical composition) and qualitative (e.g. color) data in the same analysis.

After creating analysis files and carrying out standard data quality and descriptive analyses, our suggested analysis strategy is the following: first, use these methods to evaluate whether the data available are sufficient to distinguish among sources.  Second, use a random forest analysis to order the characteristics according to their importance in distinguishing among sources.  Third, supply these characteristics in the specified order to a classification tree model, in order to predict the source of each artifact.  Finally, if all characteristics used are quantitative, plot the artifact points on a 2-dimensional principal component plot with the convex hulls of the source points, to identify artifacts for which the predicted sources are not plausible.

The final section of this vignette contains useful summary information on the functions in this package, including arguments that are standard among the functions, names of components of the list returned by each function, and instructions for selecting points of interest in some of the plots.  We also provide information on the differences between interacting with plots in base R versus Rstudio.  For those using Rstudio, we provide simple instructions for running the example code for a function.

## Example data

The example data available with this package are data on obsidian from five sources in the Jemez caldera in northern New Mexico and on obsidian artifacts from sites in the Pojoaque Valley east of that caldera.  For both the source and artifact samples, our example analyses use five elements (rubidium, Rb; strontium, Sr; ytterbium, Y; niobium, Nb; and zirconium, Zr; numerical values are in parts per million) analyzed by Steven Shackley using x-ray fluorescence.  These are the elements that Shackley uses to distinguish among sources and predict the source of an artifact.  The source data are publicly available (http://www.swxrflab.net/jemez.htm).  This site also contains discussions of the geology of the source sites.

We selected data on 91 artifacts from approximately 450 artifacts collected under the supervision of James Moore, Office of Archaeological Studies, New Mexico state government.  Shackley predicted the sources of these artifacts; all except two were from the Jemez caldera (we omitted those two from consideration); we randomly reassigned the sources of a proportion of the artifacts we selected in order to introduce misclassification.  These artifact data have not been published.

The artifacts were analyzed in Shackley’s lab in Albuquerque, New Mexico, using different instrumentation than that used for the sources.  For a discussion, see
http://www.swxrflab.net/labfees.htm#QuanX%20Energy-Dispersive%20X-Ray%20Fluorescence%20Spectrometer%20(EDXRF).  For a more general discussion of Shackley’s procedures, see http://www.swxrflab.net/swobsrcs.htm.

The five obsidian sources are coded as A, B, C, D, and E in the source and artifact data sets (ObsidianSources and ObsidianArtifacts, respectively).  The source (or predicted source for an artifact) is in the character-valued variable Code.  Each data set contains the variables Code, the five elements (as numeric variables, with variable names the element symbols), and a character-valued variable ID containing an artificial lab ID.  For example, the first few rows of ObsidianSources are in Table 1.

```{r}
library(karon)
data(ObsidianSources)
knitr::kable(ObsidianSources[1:6,], caption="Table 1.  Sample of the data in the file ObsidianSources.")
#
```
## Analysis file creation and check

The following procedure produces an analysis file from excel data files, with the first row of each file containing the name of the variable represented by the data in that column.  All files must have the same name for an element; if some files use “Zr” and others use “Zr “, the latter becomes “Zr.” when an R object is created.  Also, variable names in R are case-sensitive, so “Zr” and “ZR” are different variables.  In addition, each value of an element analysis must be numeric; the value “< 0” is not allowed.  Missing values should be left blank.

To create an R object (a data frame) from an excel file, use the R command

object_name <- read.csv(file = “file name”)

Other R functions could also be used, including read.table(); see the documentation for read.csv (obtained by entering ?read.csv at the R prompt) for alternatives and details.

If it is necessary to combine several data files into an analysis file, use the function fn.data() to create an R data frame from each of the individual R objects.  For example, Shakley’s website contains an excel file with data for individual sources.  After creating an R data frame for a source using e.g. read.csv(), the element dataOut of the list created by fn.data() is a data frame with the analysis variables in a specified sequence; to create this new data frame, use the command

new_object_name <- fn.data(data=object_name)$dataOut

where object_name is the data frame created from read.csv(); alternatively, save the result of fn.data() as an R object (a list), and create the new data frame with the command

new_object_name <-list_name$dataOut

These data frames can then be combined into an analysis data frame using the R function rbind().  If it may be necessary to repeat this operation for a number of source files, it may be useful to define an R function  

fn.combine <- function()
rbind(data1, data2,…..)

This function will return the result of the rbind() operation to define a data frame, such as

analysisFile <- fn.combine()

After creating the analysis file, use 

fn.CheckData(data=analysisFile, CheckDupVars, GroupVar, Groups, AnalyticVars) 

to obtain basic descriptive statistics and carry out several data checks (number of observations, number of missing values, number of negative values, duplicate observations, and descriptive statistics, by group and analysis variable).  For the data set ObsidianSources, there are no negative values for the five elements of interest and no duplicate observations.  The numbers of observations by source and element are in Table 2.

```{r}
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
CheckData<-fn.CheckData(data=ObsidianSources,CheckDupVars=c("Code","ID"),GroupVar="Code",Groups="All",AnalyticVars=analyticVars)
knitr::kable(CheckData$Nvalues, caption="Table 2.  Number of values for each element from each Jemez source.")
#
```
An example of the descriptive statistics is in Table 3. 

```{r} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
CheckData<-fn.CheckData(data=ObsidianSources,CheckDupVars=c("Code","ID"),GroupVar="Code",Groups="All",AnalyticVars=analyticVars)
knitr::kable(CheckData$Summary[1:6,], caption="Table 3.  Sample of the descriptive statistics for the data frame ObsidianSources.")
#
```
The example table shows that, in the descriptive statistics data frame (Robject$Summary), there is a row for each element and each source, with a row containing missing values (NAs) between elements.  If this data frame is written to an excel file, those rows between elements can be converted to blank rows, to make the resulting table easier to read.

## Descriptive statistics

The package contains functions to create basic descriptive statistics tables and plots: box plots, pairs plots (a matrix of two-dimensional scatterplots), and coefficients of variation and correlations between pairs of elements.

Box plots, created by the function fn.BoxPlots(), are useful for comparing the distributions of an element among sources and for identifying outlying values.  The figure shows box plots for four elements for the source data.  To understand the information in a plot, look at the plot of zirconium at source C.  The heavy line, at the narrow part of the notch, is the median of the zirconium values at C.  The top and bottom of the box represent the 25th and 75th percentiles of the data, respectively (the quantiles).  The notches define a 95% confidence interval for the median; if these do not overlap for two sources, it is plausible that those sources have significantly different medians.  (These plots yield warning messages that some of the notches should extend beyond the quartiles; these messages can be ignored.) Vertical dashed lines from the quartiles to a larger or smaller value with a horizontal line define the ranges of “adjacent values”.  The largest adjacent value is the largest value less than or equal to the upper quartile plus 1.5 times the interquartile range (the upper quartile minus the lower quartile).  The smallest adjacent value is the smallest value greater than or equal to the lower quartile minus 1.5 times the interquartile range.  For a standard normal distribution (mean 0, standard deviation 1), the median is 0, the upper and lower quartiles are 0.68 and -0.68, respectively; the upper and lower adjacent values are 2.72 (4 x 0.68) and -2.72, respectively.  The probability that a value from standard Gaussian distribution is greater than 2.72 is 0.003.  It follows that, for Gaussian data, the probability that a value lies outside the range of adjacent values is less than 0.01.  Therefore, values outside the range of adjacent values either may be considered outliers, or the data is unlikely to be Gaussian.   

If the box and the adjacent values are approximately symmetric around the median, then the distribution of the data is approximately symmetric.  Relative variation for values for an element among sources can be evaluated by comparing heights of boxes among sources.  
the boxes   

Now look at the figure.  We see immediately that there is little variation among the values of an element at a source. There are a few outliers, but they are not far from the range of adjacent values.  There is considerable variation in the distributions of these alements among the five Jemez sources, so it should be possible to distinguish these sources using these elements.  

```{r} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr")
boxPlots<-fn.BoxPlots(data=ObsidianSources, GroupVar="Code",                Groups="All",AnalyticVars=analyticVars,Nrow=2,Ncol=2)

```
We can also create side-by-side box plots of the sources and the artifacts assigned to the sources, in order to see how well the distributions of the elements from artifacts correspond to the distributions of the corresponding sources.  The plot below shows this comparison for rubidium and niobium; the code for the artifact is .A added to the code for the assigned source (e.g., the artifacts assigned to source A have the code A.A).  We see that, for the artifacts assigned to sources A and E, the artifact values for rubidium are larger than the corresponding source values and also are more variable.  For one artifact assigned to source D, the niobium value is a clear outlier, suggesting a misclassified artifact.  For the artifacts assigned to source E, the upper quartile for niobium is only slightly larger than the median, resulting in the unusual appearance of the box.

```{r} 
library(karon)
data(ObsidianSources)
data(ObsidianArtifacts)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
ObsidianSources<-ObsidianSources[,c("Code",analyticVars)]
Artifacts <- ObsidianArtifacts[,c("Code",analyticVars)]
SourcesCode <- as.vector(ObsidianSources[,"Code"], mode="character")
ArtifactsCode <- as.vector(paste(Artifacts[,"Code"],"A",sep="."),mode="character")
Codes <- c(SourcesCode, ArtifactsCode)
SourcesArtifacts <- data.frame(rbind(ObsidianSources,Artifacts)[,analyticVars], Code = Codes)
boxPlots<-fn.BoxPlots(data=SourcesArtifacts, GroupVar="Code", Groups="All",AnalyticVars=c("Rb","Nb"),Nrow=2,Ncol=1)

```

## Pairs plots

A pairs plot, useful in visualizing the relations between pair of variables, is a matrix containing bivariate scatter plots for all pairs of quantitative variables.  The matrix is symmetric, so corresponding plots across the main diagonal interchange the horizontal and vertical axes.  Each plot contains a robust locally weighted line to describe the trend in the data.  This line is obtained from the R function `lowess()` which computes a predicted value at each abscissa (x-value) from a regression based on a fraction of the data, with weights decreasing at abscissas farther from the point, and rejecting outliers (see e.g. Chambers et al., 1983, Chapter 4).  This function tends to try to have the line go through or near the points with the largest and smallest abscissas; therefore the behavior at the ends of the plot is not reliable.  The line is obtained from the function `panel.smooth()` within the call to `pairs()` with an argument span.  The line (smoother) will not be useful if there are very few values; an example plot demonstrates that trends may show useful results even with 15 values.  

The plot is obtained with the function `fn.PairsPlot()` and the arguments data, GroupVar, Groups, AnalyticVars, and Span.  All except Span have the same meanings as in the function fn.BoxPlot.  Span is a value between 0 and 1 (not equal to 0) defining the proportion of data used to estimate robust smooth; the function is written with a default value of 2/3, which can be changed.  A small value (such as 0.1) will produce plots which do not show a clear trend; a large value (close to 1) will produce plots which do not show changes in trends.  Running the function will produce warnings that span is not a graphical parameter; these warnings should be ignored.

The plot below of the relations between elements at source A shows that the relationships between rubidium and ytterbium, and between rubidium and zirconium, are roughly linear, but that, roughly, there is no relation between the values of strontium and those of ytterbium and zirconium.  Note that it is necessary to look at both scatterplots of the relation between a pair of variables, as limited variation in one variable and dependence of the location of the largest and smallest values of one variable can make there seem to be a relationship (see the plots of strontium as the ordinate versus ytterbium or zirconium).  

```{r} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr")
pairsPlot <- fn.PairsPlot(data=ObsidianSources, GroupVar="Code", Groups="A", AnalyticVars=analyticVars)

```

## Coefficients of variation of elements and correlations between pairs of elements

Numerical estimates of the associations between pairs of analytic variables at each source, in the form of Spearman correlation coefficients, are produced by the function fn.CV.corr().  This function also produces an estimate of the coefficient of variation (the standard deviation divided by the mean) for each variable at each source.  Because the Spearman correlation coefficient is computed from the ranks of the data, it is not affected by the numerical value of an outlier.  The coefficient of variation does use the numerical values, hence is affected by an outlier.  This function returns a list with elements with tne names CV (the coefficients of variation) and corr (the correlations).  By default, both the coefficients of variation and the correlations are rounded to two decimal places.    

Table 4 contains the correlation coefficient estimates for all pairs of variables at each source.  Each estimate describes how close the relation between a pair of variables is to strictly monotone.  If a relationship is roughly monotone, the correlation coefficient does not describe how rapidly one variable changes with respect to the other (the correlation coefficient for a linear relationship can be large even if the slope of the line describing the relationship is small).  For example, the estimated correlation between strontium and zirconium at source A is 0.51, even though the pairs plot of these elements with strontium as the ordinate shows that strontium increases slowly as zirconium increases.  The correlation table is very useful for summarizing relationships between pairs of elements at the sources: it is clear that these relationships vary substantially among these sources.

```{r} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
CV.corr<-fn.CV.corr(data = ObsidianSources, GroupVar="Code", Groups = "All", AnalyticVars=analyticVars)
knitr::kable(CV.corr$corr, caption="Table 4.  Spearman correlations among pairs of elements from the Jemez sources.")
#
```
Table 5 shows the coefficients of variation for each element at each source.  Each standard deviation is small compared to the corresponding mean (each coefficient of variation is at most 0.11, and most are 0.05 or less) except for strontium at sources C and D.  The relatively large values of the coefficient of variation for strontium at those sources is a result of the small mean strontium values at those sources: the table of descriptive statistics (not shown) gives mean values for strontium of 5 and 10, at C and D, respectively, with ranges of 0 to 8 and 2 to 11, respectively.

```{r} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
CV.corr<-fn.CV.corr(data = ObsidianSources, GroupVar="Code", Groups = "All", AnalyticVars=analyticVars)
knitr::kable(CV.corr$CV, caption="Table 5.  Coefficients of variation of the elements from the Jemez sources.")
#
```

## Two-and three-dimensional scatterplots

Two- and three-dimensional scatterplots are very useful in visualizing relationships among elements.  The user can obtain these plots from the functions fn.2dPlot and fn.3dPlot, respectively.  Several sources can be shown on a single plot, with different colors identifying the sources.  If there are a large number of sources, the user can plot medians instead of points.  For two dimensional plots, the user can show additional information: Gaussian confidence ellipses for the values from each source; the convex hull of the points from each source; either of two nonparametric curves describing the relation between elements at a source, a lowess smooth or kernel smoother.  The user can also identify points of interest (e.g. outliers) on a two-dimensional plot and create a data frame containing the data yielding those points.  The function fn.2dPlot.Gauss computes test statistics for checking that a pair of elements at a source has a bivariate Gaussian distribution.  The function fn.3dPlot.rotate allows the user to rotate a three-dimensional plot.  

Examples of two-dimensional scatterplots are below.  The first two show plots of two pairs of elements (rubidium/zirconium and rubidium/niobium) from sources A and B , with 95% and 99% Gaussian confidence ellipses; the first and second include lowess and kernel smooths for the points, respectively.  The two types of smoothers show similar trends; the kernel smooths could be made less wiggly by increasing the default value (0.3) of the span parameter KernelWidth.    

The next two plots show examples of all sources on one plot for zirconium versus rubidium.  The first of these shows the convex hull of the points at each source, with the source symbol plotted as the median of the points.  We see, for example, that the medians at sources B and D are close to the smallest values of rubidium, indicating that, for each source, there are one or more rubidium values that are large compared to most of the values.  The second plot shows the points and confidence ellipses at each source, with colors and plotting characters identifying the sources.  Requesting this plot with Identify = T allows the user to create a data set (plot.2d$data.check) containing the data generating points that may be of interest.  Table 6 shows the data generating five such points.  For example, the point from source E has a strontium value of 72, which is smaller than the value from any other sample from source E.

The confidence ellipses provide information on whether the distributions of a pair of elements differ between or among sources.  If a bivariate distribution is Gaussian and 99% confidence ellipses do not overlap between or among sources, it is plausible that the distributions of a pair of elements between or among sources do not overlap.  The function fn.2dPlot.Gauss(), discussed below, provides plots and tests to evaluate the hypothesis that a bivariate distribution is Gaussian.  To anticipate, we find that the rubidium/zirconium distribution is likely to be Gaussian at source A but not at source B.  Note that the source values roughly fill the 95% confidence ellipse at source A, but those values tend to cluster at source B, and two lie approximately on the boundary of the 99% confidence ellipse.  

```{r} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
plot.2d <- fn.2dPlot(data = ObsidianSources, GroupVar = "Code", labID = "ID", Groups = c("A","B"),
                AnalyticVars = rbind(analyticVars[c(1,4)],analyticVars[c(1,5)]), PlotEllipses=T, LowessLine=T)
```

```{r echo=FALSE} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
plot.2d <- fn.2dPlot(data = ObsidianSources, GroupVar = "Code", labID = "ID", Groups = c("A", "B"),
          AnalyticVars = rbind(analyticVars[c(1,4)], analyticVars[c(1,5)]), PlotEllipses=T, KernelSmooth=T)
```

```{r echo=FALSE} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
plot.2d <- fn.2dPlot(data = ObsidianSources, GroupVar = "Code", labID = "ID", Groups = "All", 
          AnalyticVars = analyticVars[1:2], PlotByGroup=F, PlotPoints=F, PlotMedians=T, PlotHulls=T)
```

```{r echo=FALSE} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
plot.2d <- fn.2dPlot(data = ObsidianSources, GroupVar = "Code", labID = "ID", Groups = "All",
          AnalyticVars = rbind(analyticVars[c(1,4)], analyticVars[c(1,5)]), PlotByGroup = F, PlotColors=T, PlotEllipses=T, LowessLine=T,    Identify=T)
data("sources.data.check")
knitr::kable(sources.data.check,caption="Table 6. Data generating identified points.")
#
```

The function fn.3dPlot() creates one or more three-dimensional scatterplots.  If two or more groups (sources) are shown on the same plot, different colors identify the groups, as in the example below.  The colors are specified in the parameter Colors; if several groups are shown on the same plot, there must be at least as many colors as groups; the default valule of the parameter Colors has five colors (red, black,  blue, green, purple); colors are used in that order.  The user can rotate the plot using the function fn.3dPlot.rotate().

file:///C:/Consulting/Sourcing/Vignette/scatterRbSrY.doc

```{r echo=FALSE}
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
plot3d<-fn.3dPlot(data=ObsidianSources, GroupVar="Code", Groups=c("A","B","C"), AnalyticVars=analyticVars,
                   Selections=rbind(analyticVars[1:3],analyticVars[2:4]))
```

## Evaluating bivariate normality

For a pair of variables to have a bivariate Gaussian distribution, each variable must have a univariate Gaussian distribution, and the bivariate distribution must also be Gaussian. Note that, if the bivariate distribution is not Gaussian, one of these conditions can be true (each condition is necessary but not sufficient).  The function fn.2d.Gauss() displays Q-Q plots giving the user information on whether the univariate distributions may be Gaussian. The function also computes p-values for test statistics for univariate Gaussian distributions and for a bivariate Gaussian distribution.  These p-values are in the component named pvalues in the list returned by the function.  This function also allows users to identify points of interest in the Q-Q plots (if the parameter Identify=T); the component data.check in the list returned by the function contains the data generating the points of interest.

A Q-Q plot is a quantile-quantile plot of the sample quantiles for univariate data versus the theoretical quantiles for a univariate Gaussian distribution.  Therefore, if the univariate distribution is Gaussian, the points on the plot are roughly linear.  Points with the largest values that are above the trend line indicate a distribution with a short right tail; points with the smallest values that are above the trend line indicate a distribution with a short left tail.  The opposite behaviors at the extreme indicate long tails.  See e.g. Chambers et al., Chapter 6, for a discussion of Q-Q plots. 

There are many statistical tests for univariate and bivariate Gaussian distributions.  The univariate tests have variable power against different violations of the Gaussian assumption.  The fn.2dPlot.Gauss() function implements two of the univariate tests, the Anderson-Darling and the Shapiro-Wilk tests.  Properties and definitions of many tests are in D’Agostino and Stephens, Chapter 9, Tests for the Normal Distribution.  In that chapter, D’Agostino states “the Anderson-Darling A2 test…is the EDF test we recommend for us[e]” (page 372) and “The most powerful EDF test appears to be the Anderson-Darling A2 ….  It is at times presented as being similar in power to the Shapiro-Wilk W test.” (page 404).  The p-values for those two tests are in a table produced by this function, in columns with names AD.variable_name and SW.variable_name, respectively.  The table also contains the p-values from Mardia's test for a bivariate Gaussian distribution based on skewness and kurtosis (D'Agostino and Stephens, page 409) using the mvn() function in the user-contributed package MVN.  The mvn() function may fail to return a p-value for one or both of the Mardia tests; the documentation for mvn() does not state the conditions under which this may happen.  The p-values for the tests are in the component named pvalues in the list returned by the function; examples are in Tables 7a and 7b, below.    

Q-Q plots describing the conformity of univariate Gaussian distributions for the bivariate rubidium/zirconium and rubidium/niobium bivariate distributions are below.  It is instructive to compare the Q-Q plots with and without simulation, and to compare these with the p-values in Tables 7a and 7b.  

Rubidium/zirconium:  
At source A, the Q-Q plot suggests that the distribution of rubidium has a long left tail.  However, the plot using simulation shows that the sample points are well within the 95% confidence region.  One zirconium values is large but within the 95% confidence region in the simulated plot.  The p-values for both univariate tests for both elements are relatively large.  Thus, both the results from the simulation Q-Q plots and the formal test statistics support univariate Gaussian distributions for both elements.  The p-values for both tests for bivariate normality are also relatively large.  Therefore, it is plausible that these elements have a bivariate Gaussian distribution at source A. This conclusion is consistent with the scatterplot with confidence ellipses in Figure X. 

In contrast, at source B, the Q-Q plots show that both elements have short right tails, based on four values for rubidium and three for zirconium (two of those lie on the upper boundary of the 99% confidence region).  The univariate test statistics have p-values less than 0.01.  The algorithm does not return p-values for the bivariate normality tests, but, based on the univariate plots and tests, we conclude that these elements do not have a bivariate Gaussian distribution at source B. This conclusion is also consistent with the scatterplot in Figure X.

Rubidium/niobium:
From the previous discussion, we know that rubidium has a Gaussian distribution at source A but not at source B.
At source A, niobium has one small value, but the univariate test statistics have marginally "significant" p-values.  The bivariate skewness test does not return a p-value; the kurtosis test p-value is large.  Based on these results, we may be willing to assume a bivariate Gaussian distribution for these two elements at source A.  However, the scatterplot with confidence ellipses in Figure X is of concern, as the lowess smooth is not linear, and there is one point on the boundary of the 99% ellipse.  If appears useful to look at scatterplots and smooths. 

We already know that rubidium does not have a Gaussian distribution at source B.  Since niobium has many samples with equal values, it is clear that niobium does not have a Gaussian distribution (this is confirmed by the p-values from the univariate tests).  Therefore, even though the p-values from the tests for a bivariate Gaussian distribution are large, these elements do not have a bivariate Gaussian distribution: "large" p-values from the bivariate tests are a necessary condition for bivariate normality but not sufficient. 

```{r} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
plot.2d.Gauss<-fn.2dPlot.Gauss(data=ObsidianSources, GroupVar="Code", labID="ID", Groups="A",
    AnalyticVars=c("Rb","Zr"))
plot.2d.Gauss<-fn.2dPlot.Gauss(data=ObsidianSources, GroupVar="Code", labID="ID", Groups="B",
    AnalyticVars=c("Rb","Zr"))
plot.2d.Gauss<-fn.2dPlot.Gauss(data=ObsidianSources, GroupVar="Code", labID="ID", Groups="A",
    AnalyticVars=c("Rb","Nb"))
plot.2d.Gauss<-fn.2dPlot.Gauss(data=ObsidianSources, GroupVar="Code", labID="ID", Groups="B",
    AnalyticVars=c("Rb","Nb"))
data(RbNb.pvalues)
data(RbZr.pvalues)
knitr::kable(RbZr.pvalues,caption="Table 7.a.  P-values from test statistics for a bivariable Rb/Zr distribution.")
knitr::kable(RbNb.pvalues,caption="Table 7.b.  P-values from test statistics for a bivariable Rb/Nb distribution.")
#
```
## Principal component analysis

Principal component analysis is a standard method implementing dimension reduction in order to create one or more two-dimensional plots to evaluate the separation of groups based on quantitative variables.