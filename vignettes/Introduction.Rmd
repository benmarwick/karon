---
title: "Introduction to the karon package"
author: "John Karon & Ben Marwick"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 7,
  comment = "#>"
)
```

## Introduction

A common problem in archaeology is estimating or predicting the source of the material used to form an artifact.  Archaeologists may use 2- or 3-dimensional scatterplots of artifact chemical composition data to predict the source of an artifact.  This package contains a suite of R functions to apply multivariate statistical methods to this problem, as well as functions to create analysis data sets, carry out some basic data quality checks, and perform several descriptive statistical analyses.  Results are returned as tables or plots; appropriate tables can be saved as excel files, and the user can interact with some plots to identify data observations yielding points suggesting the need for closer identification.

The package implements three multivariate statistical procedures: principal component analysis (yielding 2-dimensional plots), recursive partitioning (classification trees), and random forests.  It may be of interest that classification trees and random forests can use both quantitative (e.g. chemical composition) and qualitative (e.g. color) data in the same analysis.

After creating analysis files and carrying out standard data quality and descriptive analyses, our suggested analysis strategy is the following: first, use these methods to evaluate whether the data available are sufficient to distinguish among sources.  Second, use a random forest analysis to order the characteristics according to their importance in distinguishing among sources.  Third, supply these characteristics in the specified order to a classification tree model, in order to predict the source of each artifact.  Finally, if all characteristics used are quantitative, plot the artifact points on a 2-dimensional principal component plot with the convex hulls of the source points, to identify artifacts for which the predicted sources are not plausible.

The final section of this vignette contains useful summary information on the functions in this package, including arguments that are standard among the functions, names of components of the list returned by each function, and instructions for selecting points of interest in some of the plots.  We also provide information on the differences between interacting with plots in base R versus Rstudio.  For those using Rstudio, we provide simple instructions for running the example code for a function.

## Example data

The example data available with this package are data on obsidian from five sources in the Jemez caldera in northern New Mexico and on obsidian artifacts from sites in the Pojoaque Valley east of that caldera.  For both the source and artifact samples, our example analyses use five elements (rubidium, Rb; strontium, Sr; ytterbium, Y; niobium, Nb; and zirconium, Zr; numerical values are in parts per million) analyzed by Steven Shackley using x-ray fluorescence.  These are the elements that Shackley uses to distinguish among sources and predict the source of an artifact.  The source data are publicly available (http://www.swxrflab.net/jemez.htm).  This site also contains discussions of the geology of the source sites.

We selected data on 91 artifacts from approximately 450 artifacts collected under the supervision of James Moore, Office of Archaeological Studies, New Mexico state government.  Shackley predicted the sources of these artifacts; all except two were from the Jemez caldera (we omitted those two from consideration); we randomly reassigned the sources of a proportion of the artifacts we selected in order to introduce misclassification.  These artifact data have not been published.

The artifacts were analyzed in Shackley’s lab in Albuquerque, New Mexico, using different instrumentation than that used for the sources.  For a discussion, see
http://www.swxrflab.net/labfees.htm#QuanX%20Energy-Dispersive%20X-Ray%20Fluorescence%20Spectrometer%20(EDXRF).  For a more general discussion of Shackley’s procedures, see http://www.swxrflab.net/swobsrcs.htm.

The five obsidian sources are coded as A, B, C, D, and E in the source and artifact data sets (ObsidianSources and ObsidianArtifacts, respectively).  The source (or predicted source for an artifact) is in the character-valued variable Code.  Each data set contains the variables Code, the five elements (as numeric variables, with variable names the element symbols), and a character-valued variable ID containing an artificial lab ID.  For example, the first few rows of ObsidianSources are in Table 1.

```{r }
library(karon)
data(ObsidianSources)
knitr::kable(ObsidianSources[1:6,], caption="Table 1.  Sample of the data in the file ObsidianSources.")
#
```
## Analysis file creation and check

The following procedure produces an analysis file from excel data files, with the first row of each file containing the name of the variable represented by the data in that column.  All files must have the same name for an element; if some files use “Zr” and others use “Zr “, the latter becomes “Zr.” when an R object is created.  Also, variable names in R are case-sensitive, so “Zr” and “ZR” are different variables.  In addition, each value of an element analysis must be numeric; the value “< 0” is not allowed.  Missing values should be left blank.

To create an R object (a data frame) from an excel file, use the R command

object_name <- read.csv(file = “file name”)

Other R functions could also be used, including read.table(); see the documentation for read.csv (obtained by entering ?read.csv at the R prompt) for alternatives and details.

If it is necessary to combine several data files into an analysis file, use the function fn.data() to create an R data frame from each of the individual R objects.  For example, Shakley’s website contains an excel file with data for individual sources.  After creating an R data frame for a source using e.g. read.csv(), the element dataOut of the list created by fn.data() is a data frame with the analysis variables in a specified sequence; to create this new data frame, use the command

new_object_name <- fn.data(data=object_name)$dataOut

where object_name is the data frame created from read.csv(); alternatively, save the result of fn.data() as an R object (a list), and create the new data frame with the command

new_object_name <-list_name$dataOut

These data frames can then be combined into an analysis data frame using the R function rbind().  If it may be necessary to repeat this operation for a number of source files, it may be useful to define an R function  

fn.combine <- function()
rbind(data1, data2,…..)

This function will return the result of the rbind() operation to define a data frame, such as

analysisFile <- fn.combine()

After creating the analysis file, use 

fn.CheckData(data=analysisFile, CheckDupVars, GroupVar, Groups, AnalyticVars) 

to obtain basic descriptive statistics and carry out several data checks (number of observations, number of missing values, number of negative values, duplicate observations, and descriptive statistics, by group and analysis variable).  For the data set ObsidianSources, there are no negative values for the five elements of interest and no duplicate observations.  The numbers of observations by source and element are in Table 2.

```{r }
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
CheckData<-fn.CheckData(data=ObsidianSources,CheckDupVars=c("Code","ID"),GroupVar="Code",Groups="All",AnalyticVars=analyticVars)
knitr::kable(CheckData$Nvalues, caption="Table 2.  Number of values for each element from each Jemez source.")
#
```
An example of the descriptive statistics is in Table 3. 

```{r } 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
CheckData<-fn.CheckData(data=ObsidianSources,CheckDupVars=c("Code","ID"),GroupVar="Code",Groups="All",AnalyticVars=analyticVars)
knitr::kable(CheckData$Summary[1:6,], caption="Table 3.  Sample of the descriptive statistics for the data frame ObsidianSources.")
#
```
The example table shows that, in the descriptive statistics data frame (Robject$Summary), there is a row for each element and each source, with a row containing missing values (NAs) between elements.  If this data frame is written to an excel file, those rows between elements can be converted to blank rows, to make the resulting table easier to read.

## Descriptive statistics

The package contains functions to create basic descriptive statistics tables and plots: box plots, pairs plots (a matrix of two-dimensional scatterplots), and coefficients of variation and correlations between pairs of elements.

Box plots, created by the function fn.BoxPlots(), are useful for comparing the distributions of an element among sources and for identifying outlying values.  The figure shows box plots for four elements for the source data.  To understand the information in a plot, look at the plot of zirconium at source C.  The heavy line, at the narrow part of the notch, is the median of the zirconium values at C.  The top and bottom of the box represent the 25th and 75th percentiles of the data, respectively (the quantiles).  The notches define a 95% confidence interval for the median; if these do not overlap for two sources, it is plausible that those sources have significantly different medians.  (These plots yield warning messages that some of the notches should extend beyond the quartiles; these messages can be ignored.) Vertical dashed lines from the quartiles to a larger or smaller value with a horizontal line define the ranges of “adjacent values”.  The largest adjacent value is the largest value less than or equal to the upper quartile plus 1.5 times the interquartile range (the upper quartile minus the lower quartile).  The smallest adjacent value is the smallest value greater than or equal to the lower quartile minus 1.5 times the interquartile range.  For a standard normal distribution (mean 0, standard deviation 1), the median is 0, the upper and lower quartiles are 0.68 and -0.68, respectively; the upper and lower adjacent values are 2.72 (4 x 0.68) and -2.72, respectively.  The probability that a value from standard Gaussian distribution is greater than 2.72 is 0.003.  It follows that, for Gaussian data, the probability that a value lies outside the range of adjacent values is less than 0.01.  Therefore, values outside the range of adjacent values either may be considered outliers, or the data is unlikely to be Gaussian.   

If the box and the adjacent values are approximately symmetric around the median, then the distribution of the data is approximately symmetric.  Relative variation for values for an element among sources can be evaluated by comparing heights of boxes among sources.  
the boxes   

Now look at the figure.  We see immediately that there is little variation among the values of an element at a source. There are a few outliers, but they are not far from the range of adjacent values.  There is considerable variation in the distributions of these alements among the five Jemez sources, so it should be possible to distinguish these sources using these elements.  

```{r  echo=FALSE, results='hide',message=FALSE} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr")
boxPlots<-fn.BoxPlots(data=ObsidianSources, GroupVar="Code",                Groups="All",AnalyticVars=analyticVars,Nrow=2,Ncol=2)

```
We can also create side-by-side box plots of the sources and the artifacts assigned to the sources, in order to see how well the distributions of the elements from artifacts correspond to the distributions of the corresponding sources.  The plot below shows this comparison for rubidium and niobium; the code for the artifact is .A added to the code for the assigned source (e.g., the artifacts assigned to source A have the code A.A).  We see that, for the artifacts assigned to sources A and E, the artifact values for rubidium are larger than the corresponding source values and also are more variable.  For one artifact assigned to source D, the niobium value is a clear outlier, suggesting a misclassified artifact.  For the artifacts assigned to source E, the upper quartile for niobium is only slightly larger than the median, resulting in the unusual appearance of the box.

```{r echo=FALSE, results='hide',message=FALSE } 
library(karon)
data(ObsidianSources)
data(ObsidianArtifacts)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
ObsidianSources<-ObsidianSources[,c("Code",analyticVars)]
Artifacts <- ObsidianArtifacts[,c("Code",analyticVars)]
SourcesCode <- as.vector(ObsidianSources[,"Code"], mode="character")
ArtifactsCode <- as.vector(paste(Artifacts[,"Code"],"A",sep="."),mode="character")
Codes <- c(SourcesCode, ArtifactsCode)
SourcesArtifacts <- data.frame(rbind(ObsidianSources,Artifacts)[,analyticVars], Code = Codes)
boxPlots<-fn.BoxPlots(data=SourcesArtifacts, GroupVar="Code", Groups="All",AnalyticVars=c("Rb","Nb"),Nrow=2,Ncol=1)

```

## Pairs plots

A pairs plot, useful in visualizing the relations between pair of variables, is a matrix containing bivariate scatter plots for all pairs of quantitative variables.  The matrix is symmetric, so corresponding plots across the main diagonal interchange the horizontal and vertical axes.  Each plot contains a robust locally weighted line to describe the trend in the data.  This line is obtained from the R function `lowess()` which computes a predicted value at each abscissa (x-value) from a regression based on a fraction of the data, with weights decreasing at abscissas farther from the point, and rejecting outliers (see e.g. Chambers et al., 1983, Chapter 4).  This function tends to try to have the line go through or near the points with the largest and smallest abscissas; therefore the behavior at the ends of the plot is not reliable.  The line is obtained from the function `panel.smooth()` within the call to `pairs()` with an argument span.  The line (smoother) will not be useful if there are very few values; an example plot demonstrates that trends may show useful results even with 15 values.  

The plot is obtained with the function `fn.PairsPlot()` and the arguments data, GroupVar, Groups, AnalyticVars, and Span.  All except Span have the same meanings as in the function fn.BoxPlot.  Span is a value between 0 and 1 (not equal to 0) defining the proportion of data used to estimate robust smooth; the function is written with a default value of 2/3, which can be changed.  A small value (such as 0.1) will produce plots which do not show a clear trend; a large value (close to 1) will produce plots which do not show changes in trends.  Running the function will produce warnings that span is not a graphical parameter; these warnings should be ignored.

The plot below of the relations between elements at source A shows that the relationships between rubidium and ytterbium, and between rubidium and zirconium, are roughly linear, but that, roughly, there is no relation between the values of strontium and those of ytterbium and zirconium.  Note that it is necessary to look at both scatterplots of the relation between a pair of variables, as limited variation in one variable and dependence of the location of the largest and smallest values of one variable can make there seem to be a relationship (see the plots of strontium as the ordinate versus ytterbium or zirconium).  

```{r echo=FALSE, results='hide',message=FALSE } 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr")
pairsPlot <- fn.PairsPlot(data=ObsidianSources, GroupVar="Code", Groups="A", AnalyticVars=analyticVars)

```

## Coefficients of variation of elements and correlations between pairs of elements

Numerical estimates of the associations between pairs of analytic variables at each source, in the form of Spearman correlation coefficients, are produced by the function fn.CV.corr().  This function also produces an estimate of the coefficient of variation (the standard deviation divided by the mean) for each variable at each source.  Because the Spearman correlation coefficient is computed from the ranks of the data, it is not affected by the numerical value of an outlier.  The coefficient of variation does use the numerical values, hence is affected by an outlier.  This function returns a list with elements with tne names CV (the coefficients of variation) and corr (the correlations).  By default, both the coefficients of variation and the correlations are rounded to two decimal places.    

Table 4a contains the Spearman correlation coefficient estimates for all pairs of variables at each source.  Each estimate describes how close the relation between a pair of variables is to strictly monotone.  If a relationship is roughly monotone, the correlation coefficient does not describe how rapidly one variable changes with respect to the other (the correlation coefficient for a linear relationship can be large even if the slope of the line describing the relationship is small).  For example, the estimated correlation between strontium and zirconium at source A is 0.51, even though the pairs plot of these elements with strontium as the ordinate shows that strontium increases slowly as zirconium increases.  The correlation table is very useful for summarizing relationships between pairs of elements at the sources: it is clear that these relationships vary substantially among these sources.

Table 4b contains the Spearman correlation coefficients if group membership is ignored.  The high correlation between pairs of variables suggests that principal components will be a useful method for showing the variation among source groups in two dimensions.  

```{r } 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
CV.corr<-fn.CV.corr(data = ObsidianSources, GroupVar="Code", Groups = "All", AnalyticVars=analyticVars)
knitr::kable(CV.corr$corr, caption="Table 4a.  Spearman correlations among pairs of elements from the Jemez sources, by group.")
#
```

```{r } 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
CV.corr<-fn.CV.corr(data = ObsidianSources, GroupVar=" ", Groups=" ", AnalyticVars=analyticVars)
knitr::kable(CV.corr$corr, caption="Table 4b.  Spearman correlations among pairs of elements from the Jemez sources, all groups combined.")
#
```

Table 5 shows the coefficients of variation for each element at each source.  Each standard deviation is small compared to the corresponding mean (each coefficient of variation is at most 0.11, and most are 0.05 or less) except for strontium at sources C and D.  The relatively large values of the coefficient of variation for strontium at those sources is a result of the small mean strontium values at those sources: the table of descriptive statistics (not shown) gives mean values for strontium of 5 and 10, at C and D, respectively, with ranges of 0 to 8 and 2 to 11, respectively.

```{r } 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
CV.corr<-fn.CV.corr(data = ObsidianSources, GroupVar="Code", Groups = "All", AnalyticVars=analyticVars)
knitr::kable(CV.corr$CV, caption="Table 5.  Coefficients of variation of the elements from the Jemez sources.")
#
```

## Two-and three-dimensional scatterplots

Two- and three-dimensional scatterplots are very useful in visualizing relationships among elements.  The user can obtain these plots from the functions fn.2dPlot and fn.3dPlot, respectively.  Several sources can be shown on a single plot, with different colors identifying the sources.  If there are a large number of sources, the user can plot medians instead of points.  For two dimensional plots, the user can show additional information: Gaussian confidence ellipses for the values from each source; the convex hull of the points from each source; either of two nonparametric curves describing the relation between elements at a source, a lowess smooth or kernel smoother.  The user can also identify points of interest (e.g. outliers) on a two-dimensional plot and create a data frame containing the data yielding those points.  The function fn.2dPlot.Gauss computes test statistics for checking that a pair of elements at a source has a bivariate Gaussian distribution.  The function fn.3dPlot.rotate allows the user to rotate a three-dimensional plot.  

Examples of two-dimensional scatterplots are below.  The first two show plots of two pairs of elements (rubidium/zirconium and rubidium/niobium) from sources A and B , with 95% and 99% Gaussian confidence ellipses; the first and second include lowess and kernel smooths for the points, respectively.  The two types of smoothers show similar trends; the kernel smooths could be made less wiggly by increasing the default value (0.3) of the span parameter KernelWidth.    

The next two plots show examples of all sources on one plot for zirconium versus rubidium.  The first of these shows the convex hull of the points at each source, with the source symbol plotted as the median of the points.  We see, for example, that the medians at sources B and D are close to the smallest values of rubidium, indicating that, for each source, there are one or more rubidium values that are large compared to most of the values.  The second plot shows the points and confidence ellipses at each source, with colors and plotting characters identifying the sources.  Requesting this plot with Identify = T allows the user to create a data set (plot.2d$data.check) containing the data generating points that may be of interest.  Table 6 shows the data generating five such points.  For example, the point from source E has a strontium value of 72, which is smaller than the value from any other sample from source E.

The confidence ellipses provide information on whether the distributions of a pair of elements differ between or among sources.  If a bivariate distribution is Gaussian and 99% confidence ellipses do not overlap between or among sources, it is plausible that the distributions of a pair of elements between or among sources do not overlap.  The function fn.2dPlot.Gauss(), discussed below, provides plots and tests to evaluate the hypothesis that a bivariate distribution is Gaussian.  To anticipate, we find that the rubidium/zirconium distribution is likely to be Gaussian at source A but not at source B.  Note that the source values roughly fill the 95% confidence ellipse at source A, but those values tend to cluster at source B, and two lie approximately on the boundary of the 99% confidence ellipse.  

```{r echo=FALSE, results='hide',message=FALSE} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
plot.2d <- fn.2dPlot(data = ObsidianSources, GroupVar = "Code", labID = "ID", Groups = c("A","B"),
                AnalyticVars = rbind(analyticVars[c(1,4)],analyticVars[c(1,5)]), PlotEllipses=T, LowessLine=T)
```

```{r echo=FALSE, results='hide',message=FALSE} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
plot.2d <- fn.2dPlot(data = ObsidianSources, GroupVar = "Code", labID = "ID", Groups = c("A", "B"),
          AnalyticVars = rbind(analyticVars[c(1,4)], analyticVars[c(1,5)]), PlotEllipses=T, KernelSmooth=T)
```

```{r echo=FALSE, results='hide',message=FALSE} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
plot.2d <- fn.2dPlot(data = ObsidianSources, GroupVar = "Code", labID = "ID", Groups = "All", 
          AnalyticVars = analyticVars[1:2], PlotByGroup=F, PlotPoints=F, PlotMedians=T, PlotHulls=T)
```

```{r echo=FALSE, results='hide',message=FALSE} 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
plot.2d <- fn.2dPlot(data = ObsidianSources, GroupVar = "Code", labID = "ID", Groups = "All",
          AnalyticVars = rbind(analyticVars[c(1,4)], analyticVars[c(1,5)]), PlotByGroup = F, PlotColors=T, PlotEllipses=T, LowessLine=T, Identify=T)
```

```{r}
data("sources.data.check")
knitr::kable(sources.data.check,caption="Table 6. Data generating identified points.")
#
```

The function fn.3dPlot() creates one or more three-dimensional scatterplots.  If two or more groups (sources) are shown on the same plot, different colors identify the groups, as in the example below.  The colors are specified in the parameter Colors; if several groups are shown on the same plot, there must be at least as many colors as groups; the default valule of the parameter Colors has five colors (red, black,  blue, green, purple); colors are used in that order.  The user can rotate the plot using the function fn.3dPlot.rotate().

file:///C:/Consulting/Sourcing/Vignette/scatterRbSrY.doc

```{r echo=FALSE, results='hide',message=FALSE}
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
plot3d<-fn.3dPlot(data=ObsidianSources, GroupVar="Code", Groups=c("A","B","C"), AnalyticVars=analyticVars,
                   Selections=rbind(analyticVars[1:3],analyticVars[2:4]))
```

## Evaluating bivariate normality

The function fn.2dPlot.Gauss() allows the user to evaluate the hypothesis that a pair of variables has a bivariate Gaussian distribution in a group (source).  This function is also used in the function fn.pca.Gauss() to evaluate whether the first two principal components at a group have a bivariate distribution; therefore, the discussion in this section is also appropriate for a similar analysis of principal components results.  In either case, evaluating bivariate normality is useful in deciding whether a confidence ellipse is valid for describing the variation in the data at a group. 

For a pair of variables to have a bivariate Gaussian distribution, each variable must have a univariate Gaussian distribution, and the bivariate distribution must also be Gaussian. Note that, if the bivariate distribution is not Gaussian, one of these conditions can be true (each condition is necessary but not sufficient).  The function fn.2d.Gauss() displays Q-Q plots giving the user information on whether the univariate distributions may be Gaussian. The function also computes p-values for test statistics for univariate Gaussian distributions and for a bivariate Gaussian distribution.  These p-values are in the component named pvalues in the list returned by the function.  This function also allows users to identify points of interest in the univariate Q-Q plots (if the parameter Identify=T); the component data.check in the list returned by the function contains the data generating the points of interest.

A Q-Q plot is a quantile-quantile plot of the sample quantiles for univariate data versus the theoretical quantiles for a univariate Gaussian distribution.  Therefore, if the univariate distribution is Gaussian, the points on the plot are roughly linear.  Points with the largest values that are above the trend line indicate a distribution with a short right tail; points with the smallest values that are above the trend line indicate a distribution with a short left tail.  The opposite behaviors at the extreme indicate long tails.  See e.g. Chambers et al., Chapter 6, for a discussion of Q-Q plots. 

In addition to the standard Q-Q plot, the fn.2dPlot.Gauss() function implements a more informative plot.  To obtain this plot, a large number (by default, 1000) Gaussian distributions are simulated from the observed mean and variance of a data set.  The upper and lower 95% and 99% confidence bands for the points in the simulated Q-Q plots are computed and plotted, along with the observed points (in blue).  Observed points that fall within the 95% bands (and perhaps the 99% bands) should not be regarded as unusual.  Thus, the simulated plot figure provides information on whether a point that appears to depart from the trend line is unusual.  The simulated Q-Q plot is computed from the user-contributed function qqtest(); see Oldfield for a discussion and details.  The fn.2d.Gauss() function produces the simulation and standard Q-Q plots side-by-side for each data set.

If qqPlot is T, the function also produces the multivariate Q-Q plot.  This plot is based the fact that, if the data are Gaussian, the generalized distances of the points from the mean (the squared Mahalanobis distances) have a chi-squared distribution with two degrees of freedom (Johnson and Wichern, section 4.6).      

There are many statistical tests for univariate and bivariate Gaussian distributions.  The univariate tests have variable power against different violations of the Gaussian assumption.  The fn.2dPlot.Gauss() function implements two of the univariate tests, the Anderson-Darling and the Shapiro-Wilk tests.  Properties and definitions of many tests are in D’Agostino and Stephens, Chapter 9, Tests for the Normal Distribution.  In that chapter, D’Agostino states “the Anderson-Darling A2 test…is the EDF test we recommend for us[e]” (page 372) and “The most powerful EDF test appears to be the Anderson-Darling A2 ….  It is at times presented as being similar in power to the Shapiro-Wilk W test.” (page 404).  The p-values for those two tests are in a table produced by this function, in columns with names AD.variable_name and SW.variable_name, respectively.  The table also contains the p-values from three tests for a bivariate Gaussian distribution: Mardia's test based on skewness and kurtosis (D'Agostino and Stephens, page 409), and tests developed by Henze and Zirkler and by Royston. These tests use the mvn() function in the user-contributed package MVN.  The Royston test may be most appropriate for these relatively small sample sizes: Farrell, Salibian-Barrera, and Naczk comment that "A consistent and invariant test proposed by Henze and Zirkler...is found to have good power properties, particularly for sample sizes of 75 or more, while an approach suggested by Royston... performs effectively at detecting departures from MVN for smaller sample sizes."  The sample sizes and p-values for the tests are in the component named pvalues in the list returned by the function; examples are in Tables 7a and 7b, below.  [As discussed below, results from these data confirm the conclusion that the Royston and Henze-Zirkler tests should be preferred to the Mardia test.]    

Q-Q plots describing the conformity of univariate Gaussian distributions for the bivariate rubidium/zirconium and rubidium/niobium bivariate distributions are below, as well as the multivariate Q-Q plots.  It is instructive to compare the univariate Q-Q plots with and without simulation, in order to evaluate which data points clearly are not consistent with a univariate Gaussian distribution, and to compare these with the p-values in Tables 7a and 7b.  

Rubidium/zirconium:  
At source A, the Q-Q plot suggests that the distribution of rubidium has a long left tail.  However, the plot using simulation shows that the sample points are well within the 95% confidence region.  One zirconium value is large but within the 95% confidence region in the simulated plot.  The p-values for both univariate tests for both elements are relatively large.  Thus, both the results from the simulation Q-Q plots and the formal test statistics support univariate Gaussian distributions for both elements.  The multivariate Q-Q plot has a short right tail based on one value.  The p-values for all four tests for bivariate normality are also arge (at least 0.24).  Therefore, it is plausible that these elements have a bivariate Gaussian distribution at source A. This conclusion is consistent with the scatterplot with confidence ellipses in Figure X. 

In contrast, at source B, the Q-Q plots show that both elements have short right tails, based on four values for rubidium and three for zirconium (two of those lie on the upper boundary of the 99% confidence region).  The univariate test statistics have p-values less than 0.01.  The multivariate Q-Q plot does not appear linear.  All four multivariate tests yield small p-values (at most 0.005).  We conclude that these elements do not have a bivariate Gaussian distribution at source B. This conclusion is also consistent with the scatterplot in Figure X.

Rubidium/niobium:
From the previous discussion, we know that rubidium has a Gaussian distribution at source A but not at source B.
At source A, niobium has one small value, but the univariate test statistics have marginally "significant" p-values.  The multivariate Q-Q plot has a short right tail based on one value.  The Royston test yields a p-value of 0.065.  Based on these results, we may be willing to assume a bivariate Gaussian distribution for these two elements at source A.  However, the scatterplot with confidence ellipses in Figure X is of concern, as the lowess smooth is not linear, and there is one point on the boundary of the 99% ellipse.  If appears useful to look at scatterplots and smooths. 

We already know that rubidium does not have a Gaussian distribution at source B.  Since niobium has many samples with equal values, it is clear that niobium does not have a Gaussian distribution (this is confirmed by the p-values from the univariate tests).  The Henze-Zirkler and Royston tests properly reject bivariate normality, even though the Mardia tests do not.  The same phenomenom occurs at sources C, D, and E.  This reinforces the preference for these tests. 

```{r echo=FALSE, results='hide',message=FALSE } 
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
plot.2d.Gauss<-fn.2dPlot.Gauss(data=ObsidianSources, GroupVar="Code", labID="ID", Groups="A",
    AnalyticVars=c("Rb","Zr"))
plot.2d.Gauss<-fn.2dPlot.Gauss(data=ObsidianSources, GroupVar="Code", labID="ID", Groups="B",
    AnalyticVars=c("Rb","Zr"))
plot.2d.Gauss<-fn.2dPlot.Gauss(data=ObsidianSources, GroupVar="Code", labID="ID", Groups="A",
    AnalyticVars=c("Rb","Nb"))
plot.2d.Gauss<-fn.2dPlot.Gauss(data=ObsidianSources, GroupVar="Code", labID="ID", Groups="B",
    AnalyticVars=c("Rb","Nb"))
```

```{r}
plot.2d.Gauss<-fn.2dPlot.Gauss(data=ObsidianSources, GroupVar="Code", labID="ID", Groups="All",
    AnalyticVars=c("Rb","Zr"),qqPlot=F)
knitr::kable(plot.2d.Gauss$pvalues,caption="Table 7.a.  P-values from test statistics for a bivariable Rb/Zr distribution.")
#
```
```{r}
plot.2d.Gauss<-fn.2dPlot.Gauss(data=ObsidianSources, GroupVar="Code", labID="ID", Groups="All",
    AnalyticVars=c("Rb","Nb"),qqPlot=F)
knitr::kable(plot.2d.Gauss$pvalues,caption="Table 7.b.  Samples sizes and p-values from test statistics for a bivariable Rb/Nb distribution.")
#
```
## Principal component analysis

Principal component analysis is a standard method implementing dimension reduction in order to create one or more two-dimensional plots to evaluate the separation of groups based on quantitative variables.  The function fn.pca() in this package does a principal components analysis after standardizing the data (so that each variable has mean zero and variance 1), plots the first two principal components (with the options in in fn.2dPlot()), and returns a list with the computed values of interest (defined below).  The user can also produce a scree plot, showing the variances for each principal component, and, if the analysis is by group, box plots of the values of the component for each group for the first two components.  An option for the principal components plot is to show confidence ellipses around the points from each group (source).  As in fn.2dPlot.Gauss(), the parameter Identify=T in this function allows the user to identify points of interest.  The function fn.pca.Gauss() provides information to evaluate whether the points from each group have a bivariate Gaussian distribution, with the same information provided by the function fn.2dPlot.Gauss().

The function returns a list with numerical information on the analysis.  The component named variances is a data frame with the standard deviation, proportion of total variance, and cumulative proportion of variance for each component (see Table 8).  The component named weights contains, for each component, the weight for each predictor variable (for the obsidian data, each element) used to compute the value of the component for that variable; these weights are standardized so that the sum of the squares of the weights for each component is 1 (see Table 9 for the results for this analysis).  The components named Predicted and DataPlusPredicted contain the predicted principal component values and the original data plus the predicted values, respectively, for the data used in the analysis (see Table 10 for a partial table for the obsidian source data).  In addition, if the user identifies points of interest, the data generating those points are in the component data.check. 

For the obsidian data, the scree plot (Figure x) shows that, of the five principal components, the first two components explain nearly all the variance in the source data.  Table 8 shows that the first two components explain 96% of the total variance.  A common rule of thumb is that the components used should explain at least 90% of the total variance.  Therefore, we should be able to draw valid conclusions using the first two components.

The box plots (Figure y) show that the first component separates the five Jemez sources, except possibly for sources A and D (their box plots overlap slightly).  The box plots for A and D are widely separated by the second component: we should be able to separate the sources in a principal components analysis.

The table of weights (Table 9) shows that the first principal component is essentially strontium minus the average of the other four elements (all weights for a component can be multiplied by -1 without affecting the results).  The second component is harder to describe, but strontium and zirconium have the greatest weights.

Figure z containts the plot of the first two principal components by group with 95% and 99% confidence ellipses.  The groups are very well separated, so the fact that the confidence ellipses do not overlap is not necessary to conclude that we can separate the sources based on the principal component analysis (we would need to be concerned about the validity of the assumption that the bivariate distribution is Gaussian at each source if we wanted to use the confidence ellipses).  Note that sources A and D do overlap with respect to the first component, as predicted by the box plot, and that A and E nearly overlap, as shown by that plot.

```{r  echo=FALSE, results='hide',message=FALSE}
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
save.pca <- fn.pca(data=ObsidianSources, labID="ID", GroupVar="Code",Groups="All", AnalyticVars=analyticVars, pcPlot=F, ScreePlot=T)
```

```{r  echo=FALSE, results='hide',message=FALSE}
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
save.pca <- fn.pca(data=ObsidianSources, labID="ID", GroupVar="Code",Groups="All", AnalyticVars=analyticVars, pcPlot=F, BoxPlots=T)
```
```{r  echo=FALSE, results='hide',message=FALSE}
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
save.pca <- fn.pca(data=ObsidianSources, labID="ID", GroupVar="Code",Groups="All", AnalyticVars=analyticVars) 
```
```{r}
save.pca <- fn.pca(data=ObsidianSources, labID="ID", GroupVar="Code",Groups="All", AnalyticVars=analyticVars,
                   pcPlot=F)
knitr::kable(save.pca$Summary$importance, caption="Table 8.  Proportions of variance explained.")
knitr::kable(save.pca$weights, caption="Table 9.  Weights for each principal component.")
knitr::kable(head(save.pca$DataPlusPredicted), caption="Table 10.  Original data and principal components.")
#
```
```{r}
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
pca.Gauss <- fn.pca.Gauss(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars, qqPlot=F)
knitr::kable(pca.Gauss$p.values, caption="Table 11. P-values from test statistics for a bivariate Gaussian distribution of the first two principal components at each obsidian source.")
#
```
## Classification trees (recursive partitioning)

A classification tree is obtained from recursive partitioning when the target (dependent) variable is categorical (a regression tree has a continuous dependent variable).  A tree model does a succession of binary splits, so that the result is similar to a botanical key (when there are only two subgroups at each decision point).  The method uses a criterion of variation within a group to develop a tree structure for which the terminal groups (nodes) are as uniform as possible, subject to criteria such as a group defined by a split must contain a minimal number of data records in order to be split (the default is 20, which the user can change using the argument minSplit), or that any further splits give improvement in the variation among the terminal groups that is less than a specified minimum (the user can control this using the function argument cP). Note that the requirement of a minimum group size indicates that classification trees should not be used when data sets are too small (an imprecise criterion!). 

The classification tree procedure has a number of desirable properties:
it is nonparametric (data need not be transformed before being offered to the model, and outliers do not affect the results);
the predictor variables can be continuous, categorical, or both;
the procedure used yields a prediction error by using cross-validation (defined below);
the procedure allows some missing values (see below);
if the number of outcome groups and the number of predictor variables are both small enough, the tree can be displayed as a graph, showing the criterion for splitting at each node.

More details about recursiving partitioning are in James et al.; Strobl, Malley, and Tutz; and in Therneau and Atkinson (this latter reference has examples and explanation of output for the R rpart() function, which we use in our function fn.tree() to create classification trees).  It is important to note that, because every node decision is binary, every observation will be predicted to be in some terminal group, even if an observation does not resemble any of the other members in that group.  The rpart() function uses "surrogate variables" in the algorithm creating the classification tree if an observation contains a missing value; for an explanation of how these variabless are chosen, see Therneau and Atkinson. 

The function fn.tree() defines splits using a dichotomous criterion for a single variable (in the tree diagram, for the arm going left, the value of that variable is less than a specified amount).  The procedure starts by finding the variable in the model statement and the two subgroups defined using that variable that yields two subgroups of the outcome variable (in the case of the obsidian data, subgroups of the sources) that are as "pure" as possible.  The default criterion, used in fn.tree(), is the Gini criterion.  This splitting procedure is used at each daughter node. 

There are a number of implementations of recursive partitioning.  We used the rpart() function in the rpart library in R.  This function provides an estimate of the potential error (misclassification) in a tree using cross-validation: a subset of the data is deleted, the tree is created using the remaining data; the tree is applied to the subset, to obtain an error rate; this process is repeated, by default 10 times.  The rpart() function also has a statistical procedure for choosing the proper number of splits (recursive partitioning trees tend to be too complex, overfitting the data).   The software grows a tree with too many nodes (overfits the data), then prunes back the tree so that each split results in a specified minimum improvement (by default, 0.01) in a measure of diversity within a node.  

The tree procedure is implemented using a model statement assigned in the parameter Model containing the predictor variables separated by "+" signs.  For the examples below, the values of Model is
"Sr"+"Nb"+"Rb"+"Y"+"Zr".  The order in which the variables appear in the model statement may affect the tree produced by the algorithm (if several predictor variables yield equally good splits at a node, the procedure may use the variable that appears first in the model statement).  Therefore, model statements with variables in different orders can yield different categorical trees.  As a result, we suggest running a random forest analysis before a classification tree model, then listing the predictor variables in the tree model statement in the order of importance estimated from the random forest model.  

The function produces two plots, the classification tree and the reduction in the cross-validation error estimate with an increasing number of splits.  It returns a list with three results of the computations: a definition of the tree (incuding the split criteria; this is useful if the tree is too large to be plotted), the classification results, and the Cp table (defined below).  

In addition, the function can predict group (source) membership for new observations (artifacts).  To do so, set the parameter predictSources equal to T and the parameter predictData equal to the name of a data set with analytic data for the new observations.  In this case, the list returned by the function also has components predictedSources, a data frame with the predicted group (source) and analytic data for each new observation (also the sample ID, if there is one), and predictedTotals, a vector with the number of observations assigned to each group (source).  Missing values in an observation are also handled using surrogates.  

Figure X shows the classification tree using the obsidian source data.  Note that there are four splits, rubidium is used twice, two of the potential predictor variables are not used, and each terminal node is pure (contains observations from a single source).  [The final document will contain a plot in which the scale for each terminal box shows the code for all sources.]

```{r  echo=FALSE, results='hide',message=FALSE}
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
tree <- fn.tree(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars,
                 Model="Sr"+"Nb"+"Rb"+"Y"+"Zr", plotCp=F)
```

Figure Y shows the reduction in the crossvalidation error estimate with an increasing number of nodes.  If there are a large number of splits, the plot may show an horizontal asymptote with an increasing number of splits; this can be useful in deciding to reduce the number of splits.  By default, a decrease of 0.01 in Cp is required for a node to be split; the user can change this using the argument cP.  It is not clear why the error reduction with three splits is not shown in the plot.   

```{r  echo=FALSE, results='hide',message=FALSE}
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
tree <- fn.tree(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars,
                 Model="Sr"+"Nb"+"Rb"+"Y"+"Zr", plotTree=F)
```

If the tree is too complex to display, the description of the tree is contained in the component Tree in the list produced by fn.tree().  For the obsidian sources, this object (an R list) is displayed below.  The text above the information [node), split, n, loss, yval, (yprob)] describes what is displayed: the node number, the split criterion, the number of observations at that node, a measure of loss, the predicted group membership at that node, and the proportions in each of the groups at that node.  The predicted group membership is the group with the highest proportion.  It is helpful to compare this display with the plot showing the tree.  Observe that, for a node below the root, the first indented line describes a path to the left in the split, and the value of "node)" is twice the value of "node)" in the node that has been split. The value of "node)" for the path to the right is one greater than the value for the path to the left.  This object is displayed below (for much greater detail, we encourage the reader to use the command summary(tree$Tree) after running the example code):


```{r  echo=FALSE, message=FALSE}
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
tree <- fn.tree(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars,
                 Model="Sr"+"Nb"+"Rb"+"Y"+"Zr", plotCp=F, plotTree=F)
tree$Tree
```

The component classification in the list produced by fn.tree() contains a table with the predicted sources for the data used to create the tree.  The results for the Jemez obsidian source data are in Table y.  These results reproduce the information in Figure x.  Note that, in Figure x, the terminal nodes from left to right (1 to 5) contain the observations from sources B, A, D, C, and E, in that order, which is different from the row order in Table y.

```{r, echo=FALSE, message=FALSE  }
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
tree <- fn.tree(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars,
                 Model="Sr"+"Nb"+"Rb"+"Y"+"Zr", plotTree=F, plotCp = F)
knitr::kable(tree$classification, caption = "Table y.  Classification tree results for the obsidian sources.")
#
```

Table z contains the improvement in predicted group purity, as measured by Cp, and the estimated relative error rates and theirstandard errorsobtained from cross-validation from the tree procedure using the Jemez obsidian source data. 

```{r, echo=FALSE, message=FALSE }
library(karon)
data(ObsidianSources)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
 tree <- fn.tree(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars,
                 Model="Sr"+"Nb"+"Rb"+"Y"+"Zr", plotTree=F, plotCp = F)
knitr::kable(tree$CpTable, caption = "Table z.  Cp table from the classification tree for the obsidian sources.")
#
```

The fn.tree() function will predict sources of unknowns when the parameter predictSources is T and the parameter predictData specifies the data on the unknowns.  The member of the list produced by the function named predictedSources (a data frame) contains the predicted source for each observation, the analytic variables, and, if specified, a sample ID.  An example using the obsidian artifact data is in Table zz.  The member of the list named predictedTotals is a vector with the number of unknowns predicted to be from each source; Table zz1 contains the results using the obsidian data.

```{r, echo=FALSE, message=FALSE }
library(karon)
data(ObsidianSources)
data(ObsidianArtifacts)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
save.tree <- fn.tree(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars,
   Model = "Rb"+"Sr"+"Y"+"Zr"+"Nb", predictSources=T, predictData=ObsidianArtifacts, ID="labID",
   plotTree=F, plotCp=F)
knitr::kable(save.tree$predictedSources[1:5,], caption = "Table zz.  An example of the data frame containing predicted sources of obsidian artifacts from the classification tree for the obsidian sources.")
```

```{r, echo=FALSE, message=FALSE }
library(karon)
data(ObsidianSources)
data(ObsidianArtifacts)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
save.tree <- fn.tree(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars,
   Model = "Rb"+"Sr"+"Y"+"Zr"+"Nb", predictSources=T, predictData=ObsidianArtifacts, ID="labID",
   plotTree=F, plotCp=F)
knitr::kable(save.tree$predictedTotals, caption = "Table zz1.  Predicted number of obsidian artifacts
from each source based on the classification tree for the obsidian sources.")
```

The locations of the artifact points on a principal components plot with symbols for their their predicted sources may be useful in evaluating the reliability of these predictions.  Figure zzz shows this plot for the predictions from the classification tree analysis using the function fn.pca() (the artifact from source A was removed, as it is far from the other artifacts).  The first two principal components explain 91% of the variance.  One artifact that is predicted to be from source B appears to be from source C; eight artifacts predicted to be from source C appear to be from source B.  A data frame containing the data generating these points could be obtained by setting the parameter Identify equal to T in the cal to fn.pca().

```{r, echo=FALSE, message=FALSE  }
library(karon)
data(ObsidianSources)
data(ObsidianArtifacts)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
tree <- fn.tree(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars,
                ID="labID", Model="Sr"+"Nb"+"Rb"+"Y"+"Zr", plotTree=F, plotCp = F, predictSources=T,
                predictData=ObsidianArtifacts)
predictedSources<-tree$predictedSources[tree$predictedSources[,"source"]!="A",]
save.pca <- fn.pca(data=predictedSources, labID="labID", GroupVar="source",Groups="All", AnalyticVars=analyticVars, PlotEllipses=F)
```

## Random forests

Random forests are a method for obtaining a more realistic estimate of classification error from a classification tree and for evaluating the relative importance of predictors for those trees.  The procedure repeats a classification tree analysis a large number of times (by default in this code, 500 times).  For each analysis, the code first selects a bootstrap sample of the observations (a random sample of the same size, chosen with replacement).  Because the sample is chosen with replacement, some observations will be in the sample multiple times, and some will not appear.  It can be shown that approximately 1/3 of the observations will not appear (these are known as the "out of bag" observations).  For each analysis, a classification tree is fit using a random subsample of the predictor variables; the default is to use approximately the square root of the number of predictors (the user can change this with the argument NvarUsed in the function, fn.randomForest()).  The resulting tree is used to predict the source of each of the observations not used in the analysis.  Then the error rate can be computed from the difference between the predicted and known group memberships.

Random forests can also be used to predict group memberships for observations from unknown sources.  Again, a large number of trees are grown using bootstrap data samples and a subsample of the predictors.  The group membership is predicted for each unknown.  After running all the trees, the membership for an unknown observation is defined to be the most commonly group predicted.

The function fn.randomForest() does a random forest analysis on known group membership. The user can changed the number of trees grown using the parameter Ntrees and the number of variables used with the parameter NvarUsed; the default value is NA, in which case the code chooses a value close to the square root of the number of predictor variables.  An analysis is reproducible if the random number generator used to select the bootstrap samples and the variables used has a specified starting value.  It is desirable to specify such a value, which must be a positive integer; the code has a default value of 11111. 

The fn.randomForest() function produces two plots, the estimated error rate as a function of the number of bootstrap samples, and the importance of each variable based on the mean decrease in the Gini index as a result of the split on that variable.  The error rate plot is shown below in Figure X.  For the Jemez source data, fewer than 100 samples are required to obtain the estimated out of bag error rate.  For a set of sources that are harder to differentiate, the required number of samples is likely to be much higher.

```{r  echo=FALSE, results='hide',message=FALSE}
data(ObsidianSources)
 analyticVars<-c("Rb","Sr","Y","Zr","Nb")
 save.randomForest <- fn.randomForest(data=ObsidianSources, GroupVar="Code",Groups="All",   AnalyticVars=analyticVars, NvarUsed=3, plotImportance=F)
```                                      

The variable importance plot is shown in Figure y, and the corresponding table saved in the component named "importance" by the function is in Table z.  The corresponding estimaes of variable importance using the summary() function on the Tree component of the list produced by fn.tree() are quite different: in the order of the variables in Table z, those are 24, 21, 16, 17, 23.  Note that these sum to 101, and so are normalized, whereas those estimates in Table z sum to 86. Although the magnitudes are not necessarily comparable, the ordering is different: random forests estimates strontium to be most important, while it is third most important in the fn.tree() estimate; rubidium is of modest importance using the random forests estimate, while it is most important based on the fn.tree() estimate. 

```{r  echo=FALSE, results='hide',message=FALSE}
data(ObsidianSources)
 analyticVars<-c("Rb","Sr","Y","Zr","Nb")
 save.randomForest <- fn.randomForest(data=ObsidianSources, GroupVar="Code",Groups="All",   AnalyticVars=analyticVars, NvarUsed=3, plotErrorRate=F)
```

```{r  echo=FALSE, message=FALSE}
data(ObsidianSources)
 analyticVars<-c("Rb","Sr","Y","Zr","Nb")
 save.randomForest <- fn.randomForest(data=ObsidianSources, GroupVar="Code",Groups="All",   AnalyticVars=analyticVars, NvarUsed=3, plotImportance=F, plotErrorRate=F)
knitr::kable(save.randomForest$importance, caption="Table z. Relative variable importance in a random forest model for the Jemez obsidian source data.")
#
```

The accuracy of classification from the random forest analysis is reported in the component named "confusion" returned by fn.randomForest().  Table z shows the result for the Jemez obsidian source data.  This is the same matrix as the "classification" object from fn.tree().

```{r  echo=FALSE, message=FALSE}
data(ObsidianSources)
 analyticVars<-c("Rb","Sr","Y","Zr","Nb")
 save.randomForest <- fn.randomForest(data=ObsidianSources, GroupVar="Code",Groups="All",   AnalyticVars=analyticVars, NvarUsed=3, plotImportance=F, plotErrorRate=F)
knitr::kable(save.randomForest$confusion, caption="Table z. The confusion matrix from a random forest model for the Jemez obsidian source data: accuracy of classification.")
#
```

Setting the parameter predictSources to T and specifying the data frame with information on the unknowns (artifacts) in the parameter predictData yields the predicted sources for the unknowns from a random forest analysis. Detailed information for each unknown is in the member predictedSources of the list returned by the function.  A sample of the information in this data frame is in Table z1. The predicted source, in the variable "source", is the source with the greatest origin probability; the data frame also includes the estimated probability of each source as origin.  Each probability is calculated as the number of trees predicting the unknown to be from that source, divided by the number of trees grown (by default, 500).  The data frame predictedTotals in the list returned by the function has two rows (Table z2).  The first row tabulates the number of unknowns predicted to be from each source (using the variable "source"); the second row contains the sum of the probabilities for each potential source. Note that the predictions in the first row agree with the estimates from the classification tree model in Table x.x for sources A and D but are different for B and C: 23 and 30 for B, from the tree and random forest predictions respectively, and 38 and 31 for C.  Recall that the principal component plot in Figure xxx using the predicted sources obtained from the tree model suggested that one artifact predicted to be from B was in fact from C, and eight artifacts predicted to be from C were in fact from B.  With these changes, the predictions from the tree model would agree with the predictions from the random forest analysis.    

```{r  echo=FALSE, message=FALSE}
data(ObsidianSources)
data(ObsidianArtifacts)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
save.randomForest <- fn.randomForest(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars, NvarUsed=3, plotErrorRate=F, plotImportance=F, predictSources=T,predictData=ObsidianArtifacts, plotSourceProbs=F)
knitr::kable(save.randomForest$predictedSources[1:5,], caption="Table z1. The predicted source and predicted 
  source probabilities from a random forest model for the Jemez obsidian artifact data.")
knitr::kable(save.randomForest$predictedTotals, caption="Table z2. The predicted number of artifacts from 
  each Jemez source from a random forest model for the Jemez obsidian artifact data.")
#
```

With the logical parameter plotSourceProbs set equal to T, the user obtains two box plots, in Figures z1 and z2.  The figures provide information on the frequency with which the number of analytic variables used in the random forest analysis (in this case, 3) can predict the source of an unknown correctly.

The first plot uses only the probability for the predicted source for each unknown; the second uses all other probabilities.  We ignore the single unknown prediced to be from source A.  In the first plot, the median probability is 1 for artifacts from source C and only slightly less than 1 for artifacts from sources B and D. The first quartile of probabilities of unknowns from B is smaller then those from C and D: unknowns from source B are harder to predict than from C and D using three analytic variables. 

The second plot shows that there are a few relatively large probabilities for unknowns incorrectly assigned to sources B and C.  Table z3 displays the data from the data frame predictedSources yielding the probabilities between 0.19 and 0.30 (note that a single point on the plot can represent more than one unknown).  The unknown yielding the large probability in Figure z2 for source B was predicted to be from source C; the eight unknowns yielding the large probabilties for source C were predicted to be from source B; these artifacts are likely to correspond to the apparent misclassifications by the tree model (this could be verified by creating a data frame containing the data generating those points by setting Identify equal to T when using fn.pca() to generate Figure xxx).  Figure z3 shows a principal components plot (using fn.pca()) of the artifacts predicted to be from sources B, C, and D (the artifact from A was eliminated, as its point is far from the others).  Comparison with Figure xxx for the analysis of the predictions from the tree model show that the artifacts apparently misclassified by that model are assigned the correct source by the random forest analysis. Since this is the sample plot as in Figure xxx, the first two principal components also explain 91% of the variance. 

```{r  echo=FALSE, results='hide',message=FALSE}
data(ObsidianSources)
data(ObsidianArtifacts)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
save.randomForest <- fn.randomForest(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars, NvarUsed=3, plotErrorRate=F, plotImportance=F, predictSources=T,predictData=ObsidianArtifacts, plotSourceProbs=T)
```

```{r  echo=FALSE, message=FALSE}
data(ObsidianSources)
data(ObsidianArtifacts)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
save.randomForest <- fn.randomForest(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars, ID="labID", NvarUsed=3, plotErrorRate=F, plotImportance=F, predictSources=T,predictData=ObsidianArtifacts, plotSourceProbs=F)
check<-save.randomForest$predictedSources
rows<-((check[,"B"] > 0.19) & (check[,"B"] < 0.30)) | ((check[,"C"] > 0.19) & (check[,"C"] < 0.30))
knitr::kable(check[rows,], caption="Table z3.  Selected rows with relative large probabilities of
             misclassification.")
```

```{r  echo=FALSE, results='hide', message=FALSE}
data(ObsidianSources)
data(ObsidianArtifacts)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
save.randomForest <- fn.randomForest(data=ObsidianSources, GroupVar="Code",Groups="All", ID="labID", AnalyticVars=analyticVars, NvarUsed=3, plotErrorRate=F, plotImportance=F, predictSources=T,predictData=ObsidianArtifacts, plotSourceProbs=F)
predictedSources<-save.randomForest$predictedSources[save.randomForest$predictedSources[,"source"]!="A",]
save.pca <- fn.pca(data=predictedSources, labID="labID", GroupVar="source",Groups="All", AnalyticVars=analyticVars, PlotEllipses=F)
```

## Evalution of the predictions 



The principal component plots of the predictions from the tree and random forest analysis demonstrate the utility of such a plot in evaluating predictions.  The function fn.pca.evaluation() adds these points (specified in the parameter ArtifactData) to a principal component plot with the convex hulls of the source data (specified in the parameter ArtifactData).  For accurate predictions, the points generated by the unknowns predicted to come from a source should fall within or close to the convex hull of that source.  The user specifies the known sources and predicted sources of interest with the parameters known.sources and predicted.sources, respectively.  Defining the parameter Identify to be T allows the user to create a data frame containing data generating points that may represent misclassified unknowns.  

```{r  echo=FALSE, results='hide', message=FALSE}
data(ObsidianSources)
data(ObsidianArtifacts)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
sources <- unique(ObsidianSources[,"Code"])
save.tree <- fn.tree(data=ObsidianSources, GroupVar="Code",Groups="All", AnalyticVars=analyticVars,
  Model = "Rb"+"Sr"+"Y"+"Zr"+"Nb", predictSources=T, predictData=ObsidianArtifacts, ID="labID",
  plotTree=F, plotCp=F)
pca.eval <- fn.pca.evaluation(SourceData=ObsidianSources, ArtifactData=save.tree$predictedSources,
  SourceGroup= "Code", ArtifactGroup="source",known.sources=sources, predicted.sources=sources,
  AnalyticVars=analyticVars, plotAllPoints=T, plotHullsOutsidePoints=T, plotOutsidePoints=T)
```

```{r  echo=FALSE, results='hide', message=FALSE}
data(ObsidianSources)
data(ObsidianArtifacts)
analyticVars<-c("Rb","Sr","Y","Zr","Nb")
sources <- unique(ObsidianSources[,"Code"])
save.randomForest <- fn.randomForest(data=ObsidianSources, GroupVar="Code",Groups="All",
  AnalyticVars=analyticVars, ID="labID", NvarUsed=3, plotErrorRate=F, plotImportance=F,
  predictSources=T, predictData=ObsidianArtifacts, plotSourceProbs=F)
# pca.eval <- fn.pca.evaluation(SourceData=ObsidianSources,
#   ArtifactData=save.randomForest$predictedSources, SourceGroup= "Code", ArtifactGroup="source",
#   known.sources=sources, predicted.sources=sources, AnalyticVars=analyticVars,
#  plotAllPoints=F, plotHullsOutsidePoints = F, plotOutsidePoints = T)
 ```